# -*- coding: utf-8 -*-
"""CODTECH-TASK1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fpPxwksHbYdp8coHBPLbTizB0J-klHG3
"""

import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client
import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module

# Start Dask client
client = Client(n_workers=4, threads_per_worker=1, memory_limit='4GB')

# Generate synthetic data (in practice load from Parquet/CSV)
num_rows = 10_000_000
chunk_size = 1_000_000

# Create Dask DataFrame
df = dd.from_pandas(pd.DataFrame({
    'transaction_id': range(1, num_rows + 1),
    'customer_id': np.random.randint(1, 100000, size=num_rows),
    'product_id': np.random.randint(1, 5000, size=num_rows),
    'purchase_amount': np.random.normal(100, 50, size=num_rows),
    'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=num_rows),
    'purchase_date': pd.to_datetime('today') - pd.to_timedelta(np.random.randint(0, 365, size=num_rows), unit='D')
}), chunksize=chunk_size)

# Persist in memory
df = df.persist()

# Basic statistics
print(df.describe().compute())

# Top products analysis
top_products = df.groupby('product_id')['purchase_amount'] \
    .agg(['count', 'sum']) \
    .rename(columns={'count': 'transactions', 'sum': 'revenue'}) \
    .nlargest(10, 'revenue')
print(top_products.compute())

# Time-based analysis
daily_sales = df.groupby('purchase_date')['purchase_amount'].sum().compute()

# Visualization
plt.figure(figsize=(12, 6)) # Now plt is defined and can be used
daily_sales.plot()
plt.title('Daily Revenue Trend (Dask)')
plt.xlabel('Date')
plt.ylabel('Revenue')
plt.xticks(rotation=45)
plt.show()

# Complex operation: Rolling 7-day average
# This demonstrates Dask's ability to handle window functions on large data
rolling_avg = daily_sales.rolling(7).mean()

plt.figure(figsize=(12, 6))
daily_sales.plot(label='Daily Revenue')
rolling_avg.plot(label='7-Day Moving Avg', color='red')
plt.title('Revenue Trends with Moving Average')
plt.legend()
plt.show()

# Close Dask client
client.close()

from pyspark.sql import SparkSession

# Initialize Spark Session with proper line continuation
spark = (SparkSession.builder
    .appName("EcommerceAnalysis")
    .master("local[*]")  # Use "local[*]" for local, "yarn" for cluster
    .config("spark.executor.memory", "4g")
    .config("spark.driver.memory", "4g")
    .config("spark.sql.shuffle.partitions", "8")  # For better parallelism
    .config("spark.sql.execution.arrow.pyspark.enabled", "true")  # Better Pandas integration
    .getOrCreate())

spark = (SparkSession.builder
    .appName("ClusterAnalysis")
    .master("spark://your-cluster-master:7077")  # Cluster URL
    .config("spark.executor.instances", "8")
    .config("spark.executor.cores", "4")
    .config("spark.executor.memory", "8g")
    .getOrCreate())

# Test the Spark session is working
test_df = spark.range(1, 100).toDF("id")
print(f"Spark version: {spark.version}")
print(f"Partition count: {test_df.rdd.getNumPartitions()}")
test_df.show(5)